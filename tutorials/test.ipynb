{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "import json \n",
    "import numpy as np \n",
    "\n",
    "\n",
    "qfile = Path(\"./eval_data/llava-v1.5-7b/annotations-gqa-full.json\")\n",
    "pfile = Path(\"./eval_data/llava-v1.5-7b/gqa-formatted-predictions.json\")\n",
    "\n",
    "questions = {}\n",
    "predictions = {}\n",
    "with open(qfile) as file:\n",
    "    questions = json.load(file)\n",
    "with open(pfile) as file:\n",
    "    predictions = json.load(file)\n",
    "\n",
    "for k, q in questions.items():\n",
    "    questions[k]['detailed_type'] = q['types']['detailed']\n",
    "    questions[k]['semantic_type'] = q['types']['semantic']\n",
    "    questions[k]['structural_type'] = q['types']['structural']\n",
    "\n",
    "predictions = {str(p[\"questionId\"]): p[\"prediction\"] for p in predictions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdf = pd.DataFrame.from_dict(questions, orient='index')\n",
    "pdf = pd.DataFrame.from_dict(predictions, orient='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12578\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>semantic</th>\n",
       "      <th>entailed</th>\n",
       "      <th>equivalent</th>\n",
       "      <th>question</th>\n",
       "      <th>imageId</th>\n",
       "      <th>isBalanced</th>\n",
       "      <th>groups</th>\n",
       "      <th>answer</th>\n",
       "      <th>semanticStr</th>\n",
       "      <th>annotations</th>\n",
       "      <th>types</th>\n",
       "      <th>fullAnswer</th>\n",
       "      <th>detailed_type</th>\n",
       "      <th>semantic_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>structural_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>choose</th>\n",
       "      <td>1129</td>\n",
       "      <td>1129</td>\n",
       "      <td>1129</td>\n",
       "      <td>1129</td>\n",
       "      <td>1129</td>\n",
       "      <td>1129</td>\n",
       "      <td>1129</td>\n",
       "      <td>1129</td>\n",
       "      <td>1129</td>\n",
       "      <td>1129</td>\n",
       "      <td>1129</td>\n",
       "      <td>1129</td>\n",
       "      <td>1129</td>\n",
       "      <td>1129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compare</th>\n",
       "      <td>589</td>\n",
       "      <td>589</td>\n",
       "      <td>589</td>\n",
       "      <td>589</td>\n",
       "      <td>589</td>\n",
       "      <td>589</td>\n",
       "      <td>589</td>\n",
       "      <td>589</td>\n",
       "      <td>589</td>\n",
       "      <td>589</td>\n",
       "      <td>589</td>\n",
       "      <td>589</td>\n",
       "      <td>589</td>\n",
       "      <td>589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logical</th>\n",
       "      <td>1803</td>\n",
       "      <td>1803</td>\n",
       "      <td>1803</td>\n",
       "      <td>1803</td>\n",
       "      <td>1803</td>\n",
       "      <td>1803</td>\n",
       "      <td>1803</td>\n",
       "      <td>1803</td>\n",
       "      <td>1803</td>\n",
       "      <td>1803</td>\n",
       "      <td>1803</td>\n",
       "      <td>1803</td>\n",
       "      <td>1803</td>\n",
       "      <td>1803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query</th>\n",
       "      <td>6805</td>\n",
       "      <td>6805</td>\n",
       "      <td>6805</td>\n",
       "      <td>6805</td>\n",
       "      <td>6805</td>\n",
       "      <td>6805</td>\n",
       "      <td>6805</td>\n",
       "      <td>6805</td>\n",
       "      <td>6805</td>\n",
       "      <td>6805</td>\n",
       "      <td>6805</td>\n",
       "      <td>6805</td>\n",
       "      <td>6805</td>\n",
       "      <td>6805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verify</th>\n",
       "      <td>2252</td>\n",
       "      <td>2252</td>\n",
       "      <td>2252</td>\n",
       "      <td>2252</td>\n",
       "      <td>2252</td>\n",
       "      <td>2252</td>\n",
       "      <td>2252</td>\n",
       "      <td>2252</td>\n",
       "      <td>2252</td>\n",
       "      <td>2252</td>\n",
       "      <td>2252</td>\n",
       "      <td>2252</td>\n",
       "      <td>2252</td>\n",
       "      <td>2252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 semantic  entailed  equivalent  question  imageId  \\\n",
       "structural_type                                                      \n",
       "choose               1129      1129        1129      1129     1129   \n",
       "compare               589       589         589       589      589   \n",
       "logical              1803      1803        1803      1803     1803   \n",
       "query                6805      6805        6805      6805     6805   \n",
       "verify               2252      2252        2252      2252     2252   \n",
       "\n",
       "                 isBalanced  groups  answer  semanticStr  annotations  types  \\\n",
       "structural_type                                                                \n",
       "choose                 1129    1129    1129         1129         1129   1129   \n",
       "compare                 589     589     589          589          589    589   \n",
       "logical                1803    1803    1803         1803         1803   1803   \n",
       "query                  6805    6805    6805         6805         6805   6805   \n",
       "verify                 2252    2252    2252         2252         2252   2252   \n",
       "\n",
       "                 fullAnswer  detailed_type  semantic_type  \n",
       "structural_type                                            \n",
       "choose                 1129           1129           1129  \n",
       "compare                 589            589            589  \n",
       "logical                1803           1803           1803  \n",
       "query                  6805           6805           6805  \n",
       "verify                 2252           2252           2252  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(qdf))\n",
    "print(len(qdf.groupby(\"structural_type\").size()))\n",
    "questions\n",
    "qdf.groupby('structural_type').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12578/12578 [00:00<00:00, 1172563.03it/s]\n",
      "100%|██████████| 12578/12578 [00:00<00:00, 1531866.65it/s]\n",
      "100%|██████████| 12578/12578 [00:00<00:00, 2218034.72it/s]\n",
      "100%|██████████| 12578/12578 [00:00<00:00, 2686422.02it/s]\n",
      "100%|██████████| 12578/12578 [00:00<00:00, 3281455.23it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm \n",
    "import numpy as np \n",
    "\n",
    "def get_metric(predicted: str, answer: str) -> float:\n",
    "    return 1 if predicted == answer else 0\n",
    "\n",
    "def create_correctness_for_model(type_keyword: str,types: list[str], questions: dict, predictions: dict) -> dict:\n",
    "    \"\"\"\n",
    "    \n",
    "    Args: \n",
    "        type_keyword: name of the type keyword ex) structural_type, detailed_type \n",
    "        types: list of types ex) choose, logical etc \n",
    "        questions: format should be \n",
    "            key = 'question_id' \n",
    "            value = {'answer': answer_string, 'type': type_string} should at least include these fields. \n",
    "        predictions: format should be\n",
    "            key = 'question_id' \n",
    "            value = answer_string \n",
    "    \"\"\"\n",
    "    data = {type : defaultdict(list) for type in types}\n",
    "\n",
    "    for qid, question in tqdm(questions.items()):\n",
    "        gold = question[\"answer\"]\n",
    "        predicted = predictions[qid]\n",
    "\n",
    "        data[question[type_keyword]]['correctness'].append(get_metric(predicted, gold))\n",
    "    \n",
    "    return data \n",
    "\n",
    "def collect_correctness_per_type(correctness_dict: dict[str, dict[str, list]], models: list[str]) -> dict: \n",
    "    \"\"\"\n",
    "    Returns: \n",
    "        dictionary of 'data' and 'model' \n",
    "        'data' contains correctness per each subscenario (type). Shape is (number of prompts, number of models)\n",
    "        'model' contains the name of all models \n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    data['data'] = {}\n",
    "    data['models'] = models\n",
    "\n",
    "    for ty in correctness_dict[list(correctness_dict.keys())[0]].keys():\n",
    "        data['data'][ty] = {}\n",
    "        data['data'][ty]['correctness'] = []\n",
    "\n",
    "        for model in models:\n",
    "            data['data'][ty]['correctness'].append(correctness_dict[model][ty]['correctness'])\n",
    "                \n",
    "        data['data'][ty]['correctness'] = np.array(data['data'][ty]['correctness']).T.astype(float)\n",
    "    \n",
    "    return data \n",
    "\n",
    "def prepare_responses(data, types: list[str]):\n",
    "    \"\"\" Stack all responses of different subscenarios \"\"\"\n",
    "    responses = [np.vstack([data['data'][sub]['correctness'] for sub in types]).T]\n",
    "    return np.hstack(responses)\n",
    "\n",
    "types = ['choose', 'compare', 'logical', 'query', 'verify']\n",
    "models = ['llava-v1.5-7b', 'instructblip-vicuna-7b', 'prism-clip+7b', 'prism-dinosiglip+7b', 'prism-siglip+7b']\n",
    "data = {}\n",
    "for model in models:\n",
    "    data[model] = create_correctness_for_model('structural_type',types, questions, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'choose': {'correctness': array([[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]])},\n",
       " 'compare': {'correctness': array([[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]])},\n",
       " 'logical': {'correctness': array([[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]])},\n",
       " 'query': {'correctness': array([[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]])},\n",
       " 'verify': {'correctness': array([[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]])}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['llava-v1.5-7b']['choose']['correctness']\n",
    "\n",
    "res = collect_correctness_per_type(data, models)\n",
    "final_res = prepare_responses(res, types)\n",
    "res['data']['choose']\n",
    "# final_res.shape\n",
    "\n",
    "(res['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 12578)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import * \n",
    "\n",
    "scenarios = {'gqa':types} \n",
    "scenarios_position, subscenarios_position = prepare_data(scenarios, res)\n",
    "subscenarios_position['gqa'].keys()\n",
    "\n",
    "Y = create_responses(scenarios, res)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_weights = np.ones(Y.shape[1])\n",
    "\n",
    "bm = 'gqa'\n",
    "N = len(scenarios_position[bm])\n",
    "n_sub = len(scenarios[bm])\n",
    "for sub in scenarios[bm]:\n",
    "    n_i = len(subscenarios_position[bm][sub])\n",
    "    balance_weights[subscenarios_position[bm][sub]] = N/(n_sub*n_i)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.237854842718662e-14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs1 = np.mean([Y[:,subscenarios_position[bm][sub]].mean(axis=1) for sub in scenarios[bm]], axis=0)\n",
    "accs2 = (balance_weights*Y)[:,scenarios_position[bm]].mean(axis=1)\n",
    "\n",
    "np.abs(accs1 - accs2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_bin_train = Y[:4]\n",
    "Y_bin_test = Y[4:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:30:48] config: model_type='multidim_2pl' epochs=10                cli.py:109\n",
      "           priors='hierarchical' initializers=[] dims=1 lr=0.1                  \n",
      "           lr_decay=0.9999 dropout=0.5 hidden=100 vocab_size=None               \n",
      "           log_every=200 seed=42 deterministic=True                             \n",
      "           data_path: data/irt_val_dataset.jsonlines                  cli.py:111\n",
      "           output directory: data/irt_val_model/                      cli.py:112\n",
      "[13:30:48] amortized: False                                       dataset.py:112\n",
      "[13:30:48] Vocab size: None                                       training.py:90\n",
      "           Training Model...                                          cli.py:116\n",
      "           args: {'device': 'cpu', 'num_items': 12578,           training.py:134\n",
      "           'num_subjects': 3}                                                   \n",
      "           Parsed Model Args: {'device': 'cpu', 'num_items':     training.py:147\n",
      "           12578, 'num_subjects': 3, 'priors': 'hierarchical',                  \n",
      "           'dims': 1, 'dropout': 0.5, 'hidden': 100,                            \n",
      "           'vocab_size': None}                                                  \n",
      "torch.Size([37734]) torch.Size([37734])\n",
      "Training Pyro IRT Model for 10 epochs\n",
      "┏━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "┃ Epoch ┃ Loss       ┃ Best Loss  ┃ New LR ┃\n",
      "┡━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "│ 1     │ 50030.8996 │ 50030.8996 │ 0.1000 │\n",
      "│ 10    │ 36950.6803 │ 36950.6803 │ 0.0999 │\n",
      "└───────┴────────────┴────────────┴────────┘           Train time: 0.4061121940612793                             cli.py:122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.70s/it]\n"
     ]
    }
   ],
   "source": [
    "from irt import * \n",
    "\n",
    "scenarios = {'gqa':types} \n",
    "\n",
    "Ds = [1] # Dimensions to try\n",
    "device = 'cpu' # Either 'cuda' or 'cpu' \n",
    "epochs = 10  # Number of epochs for IRT model training (py-irt default is 2000)\n",
    "lr = .1  # Learning rate for IRT model training (py-irt default is .1)\n",
    "\n",
    "val_ind = list(range(0,Y_bin_train.shape[0],5)) # Validation indices\n",
    "train_ind = [i for i in range(Y_bin_train.shape[0]) if i not in val_ind]\n",
    "\n",
    "# Saving the training dataset in the needed format\n",
    "create_irt_dataset(Y_bin_train[train_ind], 'data/irt_val_dataset.jsonlines')\n",
    "\n",
    "# Trying different Ds\n",
    "errors = []  \n",
    "errors2 = []\n",
    "\n",
    "for D in tqdm(Ds):\n",
    "    dataset_name = 'data/irt_val_dataset.jsonlines'\n",
    "    model_name = 'data/irt_val_model/'\n",
    "    \n",
    "    # Load trained IRT model parameters\n",
    "    train_irt_model(dataset_name, model_name, D, lr, epochs, device)\n",
    "    A, B, Theta = load_irt_parameters(model_name)\n",
    "    \n",
    "    # Determine seen and unseen items for validation\n",
    "    seen_items = list(range(0, Y_bin_train.shape[1], 2))\n",
    "    unseen_items = list(range(1, Y_bin_train.shape[1], 2))\n",
    "\n",
    "    # Estimate ability parameters for the validation set\n",
    "    thetas = [estimate_ability_parameters(Y_bin_train[val_ind][j][seen_items], A[:, :, seen_items], B[:, :, seen_items]) for j in range(len(val_ind))]\n",
    "\n",
    "    # Compute validation errors for each scenario and update the errors list (in the end, we give the same weight for all scenarios)\n",
    "    errors2.append([])\n",
    "    for scenario in scenarios.keys():\n",
    "        ind = [u for u in unseen_items if u in scenarios_position[scenario]]\n",
    "        errors2[-1].append(np.mean([abs((balance_weights*item_curve(thetas[j], A, B))[0,ind].mean()-Y_bin_train[val_ind][j,ind].mean()) for j in range(len(val_ind))]))\n",
    "    errors.append(np.mean(errors2[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_D = np.argmin(np.array(errors))\n",
    "D = Ds[ind_D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_irt_dataset(Y_bin_train, 'data/irt_dataset.jsonlines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:30:50] config: model_type='multidim_2pl' epochs=10                cli.py:109\n",
      "           priors='hierarchical' initializers=[] dims=1 lr=0.1                  \n",
      "           lr_decay=0.9999 dropout=0.5 hidden=100 vocab_size=None               \n",
      "           log_every=200 seed=42 deterministic=True                             \n",
      "           data_path: data/irt_dataset.jsonlines                      cli.py:111\n",
      "           output directory: data/irt_model                           cli.py:112\n",
      "[13:30:50] amortized: False                                       dataset.py:112\n",
      "[13:30:50] Vocab size: None                                       training.py:90\n",
      "           Training Model...                                          cli.py:116\n",
      "           args: {'device': 'cpu', 'num_items': 12578,           training.py:134\n",
      "           'num_subjects': 4}                                                   \n",
      "           Parsed Model Args: {'device': 'cpu', 'num_items':     training.py:147\n",
      "           12578, 'num_subjects': 4, 'priors': 'hierarchical',                  \n",
      "           'dims': 1, 'dropout': 0.5, 'hidden': 100,                            \n",
      "           'vocab_size': None}                                                  \n",
      "torch.Size([50312]) torch.Size([50312])\n",
      "Training Pyro IRT Model for 10 epochs\n",
      "┏━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "┃ Epoch ┃ Loss       ┃ Best Loss  ┃ New LR ┃\n",
      "┡━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "│ 1     │ 73079.4077 │ 73079.4077 │ 0.1000 │\n",
      "│ 10    │ 47831.1732 │ 47831.1732 │ 0.0999 │\n",
      "└───────┴────────────┴────────────┴────────┘           Train time: 0.4085240364074707                             cli.py:122\n"
     ]
    }
   ],
   "source": [
    "train_irt_model(dataset_name='data/irt_dataset.jsonlines', \n",
    "                model_name='data/irt_model', \n",
    "                D=D, lr=lr, epochs=epochs, device=device)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def get_lambda(b, v):\n",
    "    return (b**2)/(v+(b**2))\n",
    "\n",
    "number_item = 100\n",
    "\n",
    "lambds = {} \n",
    "\n",
    "for i,scenario in enumerate(scenarios.keys()):\n",
    "    v = np.var(Y_bin_train[:,scenarios_position[scenario]], axis=1).mean()\n",
    "    b = np.mean(errors2[ind_D][i]) \n",
    "    lambds[scenario] = get_lambda(b, v/(4*number_item))\n",
    "\n",
    "with open('data/lambds.pickle', 'wb') as handle:\n",
    "    pickle.dump(lambds, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Anchor points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = 'irt' # 'correct.' or 'irt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "anchor_points = {}\n",
    "anchor_weights = {}\n",
    "\n",
    "for scenario in scenarios.keys():\n",
    "\n",
    "    if clustering=='correct.':\n",
    "        X = Y_bin_train[:,scenarios_position[scenario]].T\n",
    "    elif clustering=='irt':\n",
    "        A, B, _ = load_irt_parameters('data/irt_model/')\n",
    "        X = np.vstack((A.squeeze(), B.squeeze().reshape((1,-1)))).T\n",
    "        X = X[scenarios_position[scenario]]\n",
    "    else:\n",
    "        raise NotImplementedError \n",
    "        \n",
    "    #Normalizing balance_weights, so their sum is one within each scenario\n",
    "    norm_balance_weights = balance_weights[scenarios_position[scenario]]\n",
    "    norm_balance_weights /= norm_balance_weights.sum()\n",
    "\n",
    "    # Fitting the KMeans model\n",
    "    kmeans = KMeans(n_clusters=number_item, n_init=\"auto\", random_state=random_state)\n",
    "    kmeans.fit(X, sample_weight=norm_balance_weights)\n",
    "\n",
    "    # Calculating anchor points\n",
    "    anchor_points[scenario] = pairwise_distances(kmeans.cluster_centers_, X, metric='euclidean').argmin(axis=1)\n",
    "\n",
    "    # Calculating anchor weights\n",
    "    anchor_weights[scenario] = np.array([np.sum(norm_balance_weights[kmeans.labels_==c]) for c in range(number_item)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = {'anchor_points':anchor_points,\n",
    "          'anchor_weights':anchor_weights}\n",
    "\n",
    "with open('data/anchor.pickle', 'wb') as handle:\n",
    "    pickle.dump(anchor, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scenario: gqa, avg. error: 0.022\n"
     ]
    }
   ],
   "source": [
    "for scenario in scenarios.keys():\n",
    "    Y_anchor = Y_bin_test[:,scenarios_position[scenario]][:,anchor_points[scenario]]\n",
    "    Y_hat = (Y_anchor*anchor_weights[scenario]).sum(axis=1)\n",
    "    Y_true = (balance_weights*Y_bin_test)[:,scenarios_position[scenario]].mean(axis=1)\n",
    "\n",
    "    print(f\"scenario: {scenario}, avg. error: {np.abs(Y_hat-Y_true).mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B, _ = load_irt_parameters('data/irt_model/')\n",
    "seen_items = np.hstack([np.array(scenarios_position[scenario])[anchor_points[scenario]] for scenario in scenarios.keys()]).tolist()\n",
    "unseen_items = [i for i in range(Y_bin_train.shape[1]) if i not in seen_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 122.83it/s]\n"
     ]
    }
   ],
   "source": [
    "thetas = [estimate_ability_parameters(Y_bin_test[j][seen_items], A[:, :, seen_items], B[:, :, seen_items]) for j in tqdm(range(Y_bin_test.shape[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scenario: gqa, avg. error: 0.174\n"
     ]
    }
   ],
   "source": [
    "pirt_preds = {}\n",
    "for scenario in scenarios.keys():\n",
    "\n",
    "    ind_seen = [u for u in seen_items if u in scenarios_position[scenario]]\n",
    "    ind_unseen = [u for u in unseen_items if u in scenarios_position[scenario]]\n",
    "    pirt_lambd = Y_anchor.shape[1]/len(scenarios_position[scenario])\n",
    "\n",
    "    pirt_pred = []\n",
    "    \n",
    "    for j in range(Y_bin_test.shape[0]):\n",
    "        data_part = (balance_weights*Y_bin_test)[j,ind_seen].mean()\n",
    "        irt_part = (balance_weights*item_curve(thetas[j], A, B))[0,ind_unseen].mean()\n",
    "        pirt_pred.append(pirt_lambd*data_part + (1-pirt_lambd)*irt_part) \n",
    "        \n",
    "    pirt_preds[scenario] = np.array(pirt_pred) # Predictions\n",
    "    true = (balance_weights*Y_bin_test)[:,scenarios_position[scenario]].mean(axis=1) # True performance\n",
    "    \n",
    "    print(f\"scenario: {scenario}, avg. error: {np.abs(pirt_preds[scenario]-true).mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/lambds.pickle', 'rb') as handle:\n",
    "    lambds = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scenario: gqa, avg. error: 0.022\n"
     ]
    }
   ],
   "source": [
    "preds = {}\n",
    "for scenario in scenarios.keys():\n",
    "    Y_anchor = Y_bin_test[:,scenarios_position[scenario]][:,anchor_points[scenario]]\n",
    "    preds[scenario] = (Y_anchor*anchor_weights[scenario]).sum(axis=1) # Predictions\n",
    "    true = (balance_weights*Y_bin_test)[:,scenarios_position[scenario]].mean(axis=1) # True performance\n",
    "\n",
    "    print(f\"scenario: {scenario}, avg. error: {np.abs(preds[scenario]-true).mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [0.67841155] vs True: [0.70793229]\n",
      "scenario: gqa, avg. error: 0.030\n"
     ]
    }
   ],
   "source": [
    "gpirt_preds = {}\n",
    "for scenario in scenarios.keys():\n",
    "    gpirt_preds[scenario] = lambds[scenario]*preds[scenario]  + (1-lambds[scenario])*pirt_preds[scenario]\n",
    "    true = (balance_weights*Y_bin_test)[:,scenarios_position[scenario]].mean(axis=1) # True performance\n",
    "    \n",
    "    print(f\"Prediction: {gpirt_preds[scenario]} vs True: {true}\")\n",
    "    print(f\"scenario: {scenario}, avg. error: {np.abs(gpirt_preds[scenario]-true).mean():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
