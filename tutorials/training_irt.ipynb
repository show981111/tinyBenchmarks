{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb0a714-9d15-41bc-ac06-ce5a1e448b1d",
   "metadata": {},
   "source": [
    "# Training Item Response Theory (IRT) models\n",
    "\n",
    "In this notebook, we show how to train your own Item Response Theory (IRT) models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7008d96d-1ee5-44cf-91cb-293fb3e048bf",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85d9b93-5059-416c-8a53-e3b4cc24a904",
   "metadata": {},
   "source": [
    "Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7892164d-f5bb-4cef-9f4f-685a9af85679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from irt import *\n",
    "from utils import *\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb8ce2-1851-4131-8d35-36214be71085",
   "metadata": {},
   "source": [
    "The leaderboard dataset we will use is composed by six scenarios (sub-datasets):\n",
    "1. TruthfulQA\n",
    "1. GSM8K\n",
    "1. Winogrande\n",
    "1. ARC\n",
    "1. HellaSwag\n",
    "1. MMLU\n",
    "\n",
    "MMLU is further divided into sub-scenarios (e.g., abstract algebra, anatomy, etc). Let's check scenarios and sub-scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26499fc1-2bda-44b2-9131-e78d16f7f77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e5620-bd45-4985-b390-a154843b4d6c",
   "metadata": {},
   "source": [
    "Loading leaderboard data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca68f5c-49de-4f75-92e5-de639059cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/lb.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f14f180-d322-4cd5-8d0c-4ae4fef04127",
   "metadata": {},
   "source": [
    "In this dataset, we have data from 395 models. Let's see the names of some of them below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6c4201-0675-42e5-8a7a-8cf75592e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data['models']),data['models'][:10]\n",
    "len(data['data']['harness_hendrycksTest_abstract_algebra_5']['correctness'][0]) # ['harness_arc_challenge_25']\n",
    "data['data']['harness_hendrycksTest_abstract_algebra_5']['correctness'] # 'type' -> correctness\n",
    "# len(data['data']['harness_hendrycksTest_abstract_algebra_5']['correctness'][0])\n",
    "data['models']\n",
    "data[\"data\"]['harness_hendrycksTest_abstract_algebra_5'][\"correctness\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb5c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "subscenarios_position = {}\n",
    "first = 0\n",
    "# Iterate through each chosen scenario and its subscenarios to record item positions\n",
    "for scenario in scenarios.keys():\n",
    "    if first == 0:\n",
    "        first += 1\n",
    "        continue\n",
    "    subscenarios_position[scenario] = {}\n",
    "    for sub in scenarios[scenario]:\n",
    "        subscenarios_position[scenario][sub] = []\n",
    "        for j in range(data[\"data\"][sub][\"correctness\"].shape[0]): # Per prompts \n",
    "            subscenarios_position[scenario][sub].append(i)\n",
    "            i += 1\n",
    "    break \n",
    "subscenarios_position\n",
    "# Prepare a simplified mapping of scenarios to their item positions\n",
    "# scenarios_position = {}\n",
    "# for scenario in scenarios.keys():\n",
    "#     scenarios_position[scenario] = []\n",
    "#     for key in subscenarios_position[scenario].keys():\n",
    "#         scenarios_position[scenario] += subscenarios_position[scenario][key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d8135b-e9ec-468a-85a7-3cd3fc3d31fe",
   "metadata": {},
   "source": [
    "Below, we will process the data so all correctness scores (for all scenarios) are stored in $Y$. The dictionaries `scenarios_position` and `subscenarios_position` give the position of scenarios/subscenarios correctness scores in $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee09c25b-2dc4-4403-a972-9fb05cfe917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios_position, subscenarios_position = prepare_data(scenarios, data)\n",
    "Y = create_responses(scenarios, data)\n",
    "Y.shape\n",
    "\n",
    "# scenarios_position['mmlu']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e002485a-1e82-409b-aaf2-ddb6a82bc315",
   "metadata": {},
   "source": [
    "For example, below you can see the scores for MMLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dd9649-ba75-49c0-92fe-b00d2afc252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[:,scenarios_position['mmlu']], Y[:,scenarios_position['mmlu']].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d062bc-1efe-4527-b40c-96fc295e05fe",
   "metadata": {},
   "source": [
    "For scenarios that have multiple subscenarios, it is usually the case that we want to give equal importance to individual subscenarios when computing the aggregated performance in that scenario. This is equivalent to using a weighted average when computing the aggregated performance. We will create balance_weights, a vector of weights to help us compute those weighted averages. These weights will be different than one only for MMLU, which is the only scenario with multiple subscenarios.\n",
    "\n",
    "We will use this when choosing the IRT dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c2e717-c575-4b0c-8067-15574c3dde3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_weights = np.ones(Y.shape[1])\n",
    "\n",
    "N = len(scenarios_position['mmlu'])\n",
    "n_sub = len(scenarios['mmlu'])\n",
    "for sub in scenarios['mmlu']:\n",
    "    n_i = len(subscenarios_position['mmlu'][sub])\n",
    "    balance_weights[subscenarios_position['mmlu'][sub]] = N/(n_sub*n_i)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4dd3af-2d50-4ed5-bc36-09acf3f987fc",
   "metadata": {},
   "source": [
    "We can see below that first averaging within subscenarios and then computing a simple average is equivalent to using a weighted average from the beginning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85903718-e30c-433b-b407-74fc9ebb05eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs1 = np.mean([Y[:,subscenarios_position['mmlu'][sub]].mean(axis=1) for sub in scenarios['mmlu']], axis=0)\n",
    "accs2 = (balance_weights*Y)[:,scenarios_position['mmlu']].mean(axis=1)\n",
    "\n",
    "np.abs(accs1 - accs2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d106b620-7fe0-49bb-a8ac-3a946c15f751",
   "metadata": {},
   "source": [
    "## Training IRT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844c4412-ae69-4184-b106-191a1c151736",
   "metadata": {},
   "source": [
    "Let's split the data in train and test (recent models are placed in the test set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9874c5-7cb5-425b-8c41-9a87d7615ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = Y[:100]\n",
    "Y_train = Y[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e856d5-3778-417f-b4e9-77cfc9f6e6a1",
   "metadata": {},
   "source": [
    "To train the IRT model, we first need to binarize the values in $Y$ because correctness is not binary for TruthfulQA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739556a1-fd5c-4edb-8fd2-2e0914377eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_bin_train = np.zeros(Y_train.shape)\n",
    "Y_bin_test = np.zeros(Y_test.shape)\n",
    "\n",
    "cs = np.linspace(0.01,.99,100)  # Threshold values to consider\n",
    "for scenario in scenarios.keys():\n",
    "    ind = scenarios_position[scenario]\n",
    "    # Find the best threshold value that minimizes the difference between averages\n",
    "    c = cs[np.argmin([np.mean((np.abs((Y_train[:,ind]>c).mean(axis=1)-Y_train[:,ind].mean(axis=1)))) for c in tqdm(cs)])]\n",
    "    # Apply the threshold to train and test responses\n",
    "    Y_bin_train[:,ind] = (Y_train[:,ind]>c).astype(int)\n",
    "    Y_bin_test[:,ind] = (Y_test[:,ind]>c).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964dbee2-3607-4f6a-b287-29f12b5e20d5",
   "metadata": {},
   "source": [
    "Choosing the dimension for the IRT model based on a simple validation heuristic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9999beb9-9796-45ea-993b-cd293d343002",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ds = [5,10] # Dimensions to try\n",
    "device = 'cuda' # Either 'cuda' or 'cpu' \n",
    "epochs = 2000  # Number of epochs for IRT model training (py-irt default is 2000)\n",
    "lr = .1  # Learning rate for IRT model training (py-irt default is .1)\n",
    "\n",
    "val_ind = list(range(0,Y_bin_train.shape[0],5)) # Validation indices\n",
    "train_ind = [i for i in range(Y_bin_train.shape[0]) if i not in val_ind]\n",
    "\n",
    "# Saving the training dataset in the needed format\n",
    "create_irt_dataset(Y_bin_train[train_ind], 'data/irt_val_dataset.jsonlines')\n",
    "\n",
    "# Trying different Ds\n",
    "errors = []  \n",
    "errors2 = []\n",
    "\n",
    "for D in tqdm(Ds):\n",
    "    dataset_name = 'data/irt_val_dataset.jsonlines'\n",
    "    model_name = 'data/irt_val_model/'\n",
    "    \n",
    "    # Load trained IRT model parameters\n",
    "    train_irt_model(dataset_name, model_name, D, lr, epochs, device)\n",
    "    A, B, Theta = load_irt_parameters(model_name)\n",
    "    \n",
    "    # Determine seen and unseen items for validation\n",
    "    seen_items = list(range(0, Y_bin_train.shape[1], 2))\n",
    "    unseen_items = list(range(1, Y_bin_train.shape[1], 2))\n",
    "\n",
    "    # Estimate ability parameters for the validation set\n",
    "    thetas = [estimate_ability_parameters(Y_bin_train[val_ind][j][seen_items], A[:, :, seen_items], B[:, :, seen_items]) for j in range(len(val_ind))]\n",
    "\n",
    "    # Compute validation errors for each scenario and update the errors list (in the end, we give the same weight for all scenarios)\n",
    "    errors2.append([])\n",
    "    for scenario in scenarios.keys():\n",
    "        ind = [u for u in unseen_items if u in scenarios_position[scenario]]\n",
    "        errors2[-1].append(np.mean([abs((balance_weights*item_curve(thetas[j], A, B))[0,ind].mean()-Y_train[val_ind][j,ind].mean())for j in range(len(val_ind))]))\n",
    "    errors.append(np.mean(errors2[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbdc2b3-fb4e-4075-b1df-a747ce7d68e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_D = np.argmin(np.array(errors))\n",
    "D = Ds[ind_D]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8680d6e6-1ec2-4a24-a898-f29bd5ec109e",
   "metadata": {},
   "source": [
    "Saving the training dataset in the needed format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a80e3b-8e0c-402f-8263-7e178b976bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_irt_dataset(Y_bin_train, 'data/irt_dataset.jsonlines')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aa8114-a6b3-493c-8220-9f8976131a53",
   "metadata": {},
   "source": [
    "To train the IRT model, we use an adapted version of `py-irt` code (please check README in the tutorials directory for mode details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dcda49-4f05-4350-9dcf-d09addc8f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_irt_model(dataset_name='data/irt_dataset.jsonlines', \n",
    "                model_name='data/irt_model', \n",
    "                D=D, lr=lr, epochs=epochs, device=device)               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0f46f1",
   "metadata": {},
   "source": [
    "## Get the values for $\\lambda$ which will be used to get the gp-IRT estimates (in conjunction with anchor methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e186b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lambda(b, v):\n",
    "    return (b**2)/(v+(b**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c5c76c",
   "metadata": {},
   "source": [
    "The variable `number_item` gives the number of data points we will sample per scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9962b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_item = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2fd007",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambds = {} \n",
    "\n",
    "for i,scenario in enumerate(scenarios.keys()):\n",
    "    v = np.var(Y_train[:,scenarios_position[scenario]], axis=1).mean()\n",
    "    b = np.mean(errors2[ind_D][i]) \n",
    "    lambds[scenario] = get_lambda(b, v/(4*number_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b270a850",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/lambds.pickle', 'wb') as handle:\n",
    "    pickle.dump(lambds, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
