{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gqa': BenchmarkConfig(name='gqa', results=[EvaluationResult(prediction_file=PosixPath('data/gqa/llava-v1.5-7b/gqa-formatted-predictions.json'), model='llava-v1.5-7b'), EvaluationResult(prediction_file=PosixPath('data/gqa/instructblip-vicuna-7b/gqa-formatted-predictions.json'), model='instructblip-vicuna-7b'), EvaluationResult(prediction_file=PosixPath('data/gqa/prism-clip+7b/gqa-formatted-predictions.json'), model='prism-clip+7b'), EvaluationResult(prediction_file=PosixPath('data/gqa/prism-dinosiglip+7b/gqa-formatted-predictions.json'), model='prism-dinosiglip+7b'), EvaluationResult(prediction_file=PosixPath('data/gqa/prism-siglip+7b/gqa-formatted-predictions.json'), model='prism-siglip+7b')], question_file=PosixPath('data/gqa/questions.json'), subscenario_keyword='structural_type', models=['llava-v1.5-7b', 'instructblip-vicuna-7b', 'prism-clip+7b', 'prism-dinosiglip+7b', 'prism-siglip+7b'])}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lmm.benchmark_processors.benchmark_processor import BenchmarkConfig, EvaluationResult\n",
    "\n",
    "benchmarks = [\"gqa\"]\n",
    "models = ['llava-v1.5-7b', 'instructblip-vicuna-7b', 'prism-clip+7b', 'prism-dinosiglip+7b', 'prism-siglip+7b']\n",
    "benchmark_configurations: dict[str, BenchmarkConfig] = {}\n",
    "\n",
    "for bm in benchmarks:\n",
    "    results: list[EvaluationResult] = []\n",
    "    for model in models: \n",
    "        results.append(EvaluationResult(prediction_file=f\"./data/{bm}/{model}/gqa-formatted-predictions.json\", model=model))\n",
    "\n",
    "    benchmark_configurations[bm] = BenchmarkConfig(name = bm,results=results, question_file = f\"./data/{bm}/questions.json\", subscenario_keyword=\"structural_type\")\n",
    "\n",
    "benchmark_configurations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: ['llava-v1.5-7b', 'instructblip-vicuna-7b', 'prism-clip+7b', 'prism-dinosiglip+7b', 'prism-siglip+7b']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12578/12578 [00:00<00:00, 2041244.18it/s]\n",
      "100%|██████████| 12578/12578 [00:00<00:00, 2522156.89it/s]\n",
      "100%|██████████| 12578/12578 [00:00<00:00, 1774980.01it/s]\n",
      "100%|██████████| 12578/12578 [00:00<00:00, 2093158.06it/s]\n",
      "100%|██████████| 12578/12578 [00:00<00:00, 2251737.41it/s]\n"
     ]
    }
   ],
   "source": [
    "from lmm.generate_irt_train_data import generate_irt_train_data\n",
    "\n",
    "\n",
    "data = generate_irt_train_data(benchmark_configurations)\n",
    "scenarios_position = data.scenarios_position\n",
    "subscenarios_position = data.subscenarios_position\n",
    "scenarios = data.scenarios\n",
    "Y = data.train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "bm = \"gqa\"\n",
    "balance_weights = np.ones(Y.shape[1])\n",
    "\n",
    "N = len(scenarios_position[bm])\n",
    "n_sub = len(scenarios[bm])\n",
    "for sub in scenarios[bm]:\n",
    "    n_i = len(subscenarios_position[bm][sub])\n",
    "    balance_weights[subscenarios_position[bm][sub]] = N/(n_sub*n_i)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.295141937087465e-14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs1 = np.mean([Y[:,subscenarios_position[bm][sub]].mean(axis=1) for sub in scenarios[bm]], axis=0)\n",
    "accs2 = (balance_weights*Y)[:,scenarios_position[bm]].mean(axis=1)\n",
    "\n",
    "np.abs(accs1 - accs2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_bin_train = Y[:4]\n",
    "Y_bin_test = Y[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:10:11] config: model_type='multidim_2pl' epochs=2000              cli.py:109\n",
      "           priors='hierarchical' initializers=[] dims=5 lr=0.1                  \n",
      "           lr_decay=0.9999 dropout=0.5 hidden=100 vocab_size=None               \n",
      "           log_every=200 seed=42 deterministic=True                             \n",
      "           data_path: data/irt_val_dataset.jsonlines                  cli.py:111\n",
      "           output directory: data/irt_val_model/                      cli.py:112\n",
      "[23:10:11] amortized: False                                       dataset.py:112\n",
      "[23:10:11] Vocab size: None                                       training.py:90\n",
      "           Training Model...                                          cli.py:116\n",
      "           args: {'device': 'cpu', 'num_items': 12578,           training.py:134\n",
      "           'num_subjects': 3}                                                   \n",
      "           Parsed Model Args: {'device': 'cpu', 'num_items':     training.py:147\n",
      "           12578, 'num_subjects': 3, 'priors': 'hierarchical',                  \n",
      "           'dims': 5, 'dropout': 0.5, 'hidden': 100,                            \n",
      "           'vocab_size': None}                                                  \n",
      "torch.Size([37734]) torch.Size([37734])\n",
      "Training Pyro IRT Model for 2000 epochs\n",
      "┏━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "┃ Epoch ┃ Loss        ┃ Best Loss   ┃ New LR ┃\n",
      "┡━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "│ 1     │ 236718.4699 │ 236718.4699 │ 0.1000 │\n",
      "│ 201   │ 37013.8139  │ 29352.8804  │ 0.0980 │\n",
      "│ 401   │ 37581.4942  │ 25939.1140  │ 0.0961 │\n",
      "│ 601   │ 33519.9745  │ 25939.1140  │ 0.0942 │\n",
      "│ 801   │ 29996.2068  │ 25551.9708  │ 0.0923 │\n",
      "│ 1001  │ 31472.1394  │ 25038.7050  │ 0.0905 │\n",
      "│ 1201  │ 26635.5861  │ 24623.8062  │ 0.0887 │\n",
      "│ 1401  │ 26258.9061  │ 24520.2393  │ 0.0869 │\n",
      "│ 1601  │ 25574.9231  │ 24422.0184  │ 0.0852 │\n",
      "│ 1801  │ 24529.5866  │ 24086.2461  │ 0.0835 │\n",
      "│ 2000  │ 26094.0850  │ 24086.2461  │ 0.0819 │\n",
      "└───────┴───��─────────┴─────────────┴────────┘[23:10:32] Train time: 20.710763931274414                             cli.py:122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:22<00:22, 22.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:10:34] config: model_type='multidim_2pl' epochs=2000              cli.py:109\n",
      "           priors='hierarchical' initializers=[] dims=10 lr=0.1                 \n",
      "           lr_decay=0.9999 dropout=0.5 hidden=100 vocab_size=None               \n",
      "           log_every=200 seed=42 deterministic=True                             \n",
      "           data_path: data/irt_val_dataset.jsonlines                  cli.py:111\n",
      "           output directory: data/irt_val_model/                      cli.py:112\n",
      "[23:10:34] amortized: False                                       dataset.py:112\n",
      "[23:10:34] Vocab size: None                                       training.py:90\n",
      "           Training Model...                                          cli.py:116\n",
      "           args: {'device': 'cpu', 'num_items': 12578,           training.py:134\n",
      "           'num_subjects': 3}                                                   \n",
      "           Parsed Model Args: {'device': 'cpu', 'num_items':     training.py:147\n",
      "           12578, 'num_subjects': 3, 'priors': 'hierarchical',                  \n",
      "           'dims': 10, 'dropout': 0.5, 'hidden': 100,                           \n",
      "           'vocab_size': None}                                                  \n",
      "torch.Size([37734]) torch.Size([37734])\n",
      "Training Pyro IRT Model for 2000 epochs\n",
      "┏━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "┃ Epoch ┃ Loss        ┃ Best Loss   ┃ New LR ┃\n",
      "┡━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "│ 1     │ 306979.5071 │ 306979.5071 │ 0.1000 │\n",
      "│ 201   │ 64819.7526  │ 43915.9416  │ 0.0980 │\n",
      "│ 401   │ 34203.3167  │ 34010.9717  │ 0.0961 │\n",
      "│ 601   │ 34954.8265  │ 31779.8351  │ 0.0942 │\n",
      "│ 801   │ 33575.0909  │ 30070.9637  │ 0.0923 │\n",
      "│ 1001  │ 31105.2651  │ 30070.9637  │ 0.0905 │\n",
      "│ 1201  │ 43908.9604  │ 28984.4929  │ 0.0887 │\n",
      "│ 1401  │ 33138.4864  │ 28745.4662  │ 0.0869 │\n",
      "│ 1601  │ 32098.0001  │ 28190.1618  │ 0.0852 │\n",
      "│ 1801  │ 30838.4748  │ 27926.6124  │ 0.0835 │\n",
      "│ 2000  │ 30861.9054  │ 27707.2390  │ 0.0819 │\n",
      "└───────┴───��─────────┴─────────────┴────────┘[23:11:01] Train time: 27.681722164154053                             cli.py:122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:52<00:00, 26.21s/it]\n"
     ]
    }
   ],
   "source": [
    "from tutorials.irt import * \n",
    "from tqdm import tqdm\n",
    "\n",
    "scenarios = data.scenarios\n",
    "\n",
    "Ds = [5,10] # Dimensions to try\n",
    "device = 'cpu' # Either 'cuda' or 'cpu' \n",
    "epochs = 2000  # Number of epochs for IRT model training (py-irt default is 2000)\n",
    "lr = .1  # Learning rate for IRT model training (py-irt default is .1)\n",
    "\n",
    "val_ind = list(range(0,Y_bin_train.shape[0],5)) # Validation indices\n",
    "train_ind = [i for i in range(Y_bin_train.shape[0]) if i not in val_ind]\n",
    "\n",
    "# Saving the training dataset in the needed format\n",
    "create_irt_dataset(Y_bin_train[train_ind], 'data/irt_val_dataset.jsonlines')\n",
    "\n",
    "# Trying different Ds\n",
    "errors = []  \n",
    "errors2 = []\n",
    "\n",
    "for D in tqdm(Ds):\n",
    "    dataset_name = 'data/irt_val_dataset.jsonlines'\n",
    "    model_name = 'data/irt_val_model/'\n",
    "    \n",
    "    # Load trained IRT model parameters\n",
    "    train_irt_model(dataset_name, model_name, D, lr, epochs, device)\n",
    "    A, B, Theta = load_irt_parameters(model_name)\n",
    "    \n",
    "    # Determine seen and unseen items for validation\n",
    "    seen_items = list(range(0, Y_bin_train.shape[1], 2))\n",
    "    unseen_items = list(range(1, Y_bin_train.shape[1], 2))\n",
    "\n",
    "    # Estimate ability parameters for the validation set\n",
    "    thetas = [estimate_ability_parameters(Y_bin_train[val_ind][j][seen_items], A[:, :, seen_items], B[:, :, seen_items]) for j in range(len(val_ind))]\n",
    "\n",
    "    # Compute validation errors for each scenario and update the errors list (in the end, we give the same weight for all scenarios)\n",
    "    errors2.append([])\n",
    "    for scenario in scenarios.keys():\n",
    "        ind = [u for u in unseen_items if u in scenarios_position[scenario]]\n",
    "        errors2[-1].append(np.mean([abs((balance_weights*item_curve(thetas[j], A, B))[0,ind].mean()-Y_bin_train[val_ind][j,ind].mean()) for j in range(len(val_ind))]))\n",
    "    errors.append(np.mean(errors2[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_D = np.argmin(np.array(errors))\n",
    "D = Ds[ind_D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_irt_dataset(Y_bin_train, 'data/irt_dataset.jsonlines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:12:19] config: model_type='multidim_2pl' epochs=2000              cli.py:109\n",
      "           priors='hierarchical' initializers=[] dims=5 lr=0.1                  \n",
      "           lr_decay=0.9999 dropout=0.5 hidden=100 vocab_size=None               \n",
      "           log_every=200 seed=42 deterministic=True                             \n",
      "           data_path: data/irt_dataset.jsonlines                      cli.py:111\n",
      "           output directory: data/irt_model                           cli.py:112\n",
      "[23:12:19] amortized: False                                       dataset.py:112\n",
      "[23:12:19] Vocab size: None                                       training.py:90\n",
      "           Training Model...                                          cli.py:116\n",
      "           args: {'device': 'cpu', 'num_items': 12578,           training.py:134\n",
      "           'num_subjects': 4}                                                   \n",
      "           Parsed Model Args: {'device': 'cpu', 'num_items':     training.py:147\n",
      "           12578, 'num_subjects': 4, 'priors': 'hierarchical',                  \n",
      "           'dims': 5, 'dropout': 0.5, 'hidden': 100,                            \n",
      "           'vocab_size': None}                                                  \n",
      "torch.Size([50312]) torch.Size([50312])\n",
      "Training Pyro IRT Model for 2000 epochs\n",
      "┏━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "┃ Epoch ┃ Loss        ┃ Best Loss   ┃ New LR ┃\n",
      "┡━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "│ 1     │ 172520.8438 │ 172520.8438 │ 0.1000 │\n",
      "│ 201   │ 32720.6628  │ 32720.6628  │ 0.0980 │\n",
      "│ 401   │ 34530.1962  │ 30210.1392  │ 0.0961 │\n",
      "│ 601   │ 32851.8361  │ 29729.1361  │ 0.0942 │\n",
      "│ 801   │ 32756.1661  │ 29383.1325  │ 0.0923 │\n",
      "│ 1001  │ 31160.0140  │ 29210.7701  │ 0.0905 │\n",
      "│ 1201  │ 29998.9448  │ 29210.7701  │ 0.0887 │\n",
      "│ 1401  │ 30273.5257  │ 28881.9821  │ 0.0869 │\n",
      "│ 1601  │ 29855.1368  │ 28606.4487  │ 0.0852 │\n",
      "│ 1801  │ 31880.0690  │ 28568.2998  │ 0.0835 │\n",
      "│ 2000  │ 29692.3469  │ 28446.5102  │ 0.0819 │\n",
      "└───────┴───��─────────┴─────────────┴────────┘[23:12:42] Train time: 22.45142412185669                              cli.py:122\n"
     ]
    }
   ],
   "source": [
    "train_irt_model(dataset_name='data/irt_dataset.jsonlines', \n",
    "                model_name='data/irt_model', \n",
    "                D=D, lr=lr, epochs=epochs, device=device)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def get_lambda(b, v):\n",
    "    return (b**2)/(v+(b**2))\n",
    "\n",
    "number_item = 100\n",
    "\n",
    "lambds = {} \n",
    "\n",
    "for i,scenario in enumerate(scenarios.keys()):\n",
    "    v = np.var(Y_bin_train[:,scenarios_position[scenario]], axis=1).mean()\n",
    "    b = np.mean(errors2[ind_D][i]) \n",
    "    lambds[scenario] = get_lambda(b, v/(4*number_item))\n",
    "\n",
    "with open('data/lambds.pickle', 'wb') as handle:\n",
    "    pickle.dump(lambds, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Anchor point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = 'irt' # 'correct.' or 'irt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "anchor_points = {}\n",
    "anchor_weights = {}\n",
    "\n",
    "for scenario in scenarios.keys():\n",
    "\n",
    "    if clustering=='correct.':\n",
    "        X = Y_bin_train[:,scenarios_position[scenario]].T\n",
    "    elif clustering=='irt':\n",
    "        A, B, _ = load_irt_parameters('data/irt_model/')\n",
    "        X = np.vstack((A.squeeze(), B.squeeze().reshape((1,-1)))).T\n",
    "        X = X[scenarios_position[scenario]]\n",
    "    else:\n",
    "        raise NotImplementedError \n",
    "        \n",
    "    #Normalizing balance_weights, so their sum is one within each scenario\n",
    "    norm_balance_weights = balance_weights[scenarios_position[scenario]]\n",
    "    norm_balance_weights /= norm_balance_weights.sum()\n",
    "\n",
    "    # Fitting the KMeans model\n",
    "    kmeans = KMeans(n_clusters=number_item, n_init=\"auto\", random_state=random_state)\n",
    "    kmeans.fit(X, sample_weight=norm_balance_weights)\n",
    "\n",
    "    # Calculating anchor points\n",
    "    anchor_points[scenario] = pairwise_distances(kmeans.cluster_centers_, X, metric='euclidean').argmin(axis=1)\n",
    "\n",
    "    # Calculating anchor weights\n",
    "    anchor_weights[scenario] = np.array([np.sum(norm_balance_weights[kmeans.labels_==c]) for c in range(number_item)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = {'anchor_points':anchor_points,\n",
    "          'anchor_weights':anchor_weights}\n",
    "\n",
    "with open('data/anchor.pickle', 'wb') as handle:\n",
    "    pickle.dump(anchor, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scenario: gqa, avg. error: 0.021\n"
     ]
    }
   ],
   "source": [
    "for scenario in scenarios.keys():\n",
    "    Y_anchor = Y_bin_test[:,scenarios_position[scenario]][:,anchor_points[scenario]]\n",
    "    Y_hat = (Y_anchor*anchor_weights[scenario]).sum(axis=1)\n",
    "    Y_true = (balance_weights*Y_bin_test)[:,scenarios_position[scenario]].mean(axis=1)\n",
    "\n",
    "    print(f\"scenario: {scenario}, avg. error: {np.abs(Y_hat-Y_true).mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B, _ = load_irt_parameters('data/irt_model/')\n",
    "seen_items = np.hstack([np.array(scenarios_position[scenario])[anchor_points[scenario]] for scenario in scenarios.keys()]).tolist()\n",
    "unseen_items = [i for i in range(Y_bin_train.shape[1]) if i not in seen_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 83.07it/s]\n"
     ]
    }
   ],
   "source": [
    "thetas = [estimate_ability_parameters(Y_bin_test[j][seen_items], A[:, :, seen_items], B[:, :, seen_items]) for j in tqdm(range(Y_bin_test.shape[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scenario: gqa, avg. error: 0.010\n"
     ]
    }
   ],
   "source": [
    "pirt_preds = {}\n",
    "for scenario in scenarios.keys():\n",
    "\n",
    "    ind_seen = [u for u in seen_items if u in scenarios_position[scenario]]\n",
    "    ind_unseen = [u for u in unseen_items if u in scenarios_position[scenario]]\n",
    "    pirt_lambd = Y_anchor.shape[1]/len(scenarios_position[scenario])\n",
    "\n",
    "    pirt_pred = []\n",
    "    \n",
    "    for j in range(Y_bin_test.shape[0]):\n",
    "        data_part = (balance_weights*Y_bin_test)[j,ind_seen].mean()\n",
    "        irt_part = (balance_weights*item_curve(thetas[j], A, B))[0,ind_unseen].mean()\n",
    "        pirt_pred.append(pirt_lambd*data_part + (1-pirt_lambd)*irt_part) \n",
    "        \n",
    "    pirt_preds[scenario] = np.array(pirt_pred) # Predictions\n",
    "    true = (balance_weights*Y_bin_test)[:,scenarios_position[scenario]].mean(axis=1) # True performance\n",
    "    \n",
    "    print(f\"scenario: {scenario}, avg. error: {np.abs(pirt_preds[scenario]-true).mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/lambds.pickle', 'rb') as handle:\n",
    "    lambds = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scenario: gqa, avg. error: 0.021\n"
     ]
    }
   ],
   "source": [
    "preds = {}\n",
    "for scenario in scenarios.keys():\n",
    "    Y_anchor = Y_bin_test[:,scenarios_position[scenario]][:,anchor_points[scenario]]\n",
    "    preds[scenario] = (Y_anchor*anchor_weights[scenario]).sum(axis=1) # Predictions\n",
    "    true = (balance_weights*Y_bin_test)[:,scenarios_position[scenario]].mean(axis=1) # True performance\n",
    "\n",
    "    print(f\"scenario: {scenario}, avg. error: {np.abs(preds[scenario]-true).mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [0.7159501] vs True: [0.73548393]\n",
      "scenario: gqa, avg. error: 0.020\n"
     ]
    }
   ],
   "source": [
    "gpirt_preds = {}\n",
    "for scenario in scenarios.keys():\n",
    "    gpirt_preds[scenario] = lambds[scenario]*preds[scenario]  + (1-lambds[scenario])*pirt_preds[scenario]\n",
    "    true = (balance_weights*Y_bin_test)[:,scenarios_position[scenario]].mean(axis=1) # True performance\n",
    "    \n",
    "    print(f\"Prediction: {gpirt_preds[scenario]} vs True: {true}\")\n",
    "    print(f\"scenario: {scenario}, avg. error: {np.abs(gpirt_preds[scenario]-true).mean():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
