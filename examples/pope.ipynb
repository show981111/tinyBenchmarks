{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# In order to import custom tiny bench from example directory\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "from custom_tiny_bench.processor.benchmark_processor import (\n",
    "    BenchmarkConfig,\n",
    "    EvaluationResult,\n",
    ")\n",
    "from custom_tiny_bench.tiny_benchmark import TinyBenchmark\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    force = True)\n",
    "\n",
    "save_dir = Path(\"../data\")\n",
    "bm_configs: list[BenchmarkConfig] = [\n",
    "    BenchmarkConfig(\n",
    "        name=\"gqa\",\n",
    "        results=[\n",
    "            EvaluationResult(\n",
    "                prediction_file=\"../data/gqa/instructblip-vicuna-7b/gqa-formatted-predictions.json\",\n",
    "                model=\"instructblip-vicuna-7b\",\n",
    "            ),\n",
    "            EvaluationResult(\n",
    "                prediction_file=\"../data/gqa/llava-v1.5-7b/gqa-formatted-predictions.json\",\n",
    "                model=\"llava-v1.5-7b\",\n",
    "            ),\n",
    "            EvaluationResult(\n",
    "                prediction_file=\"../data/gqa/prism-clip+7b/gqa-formatted-predictions.json\",\n",
    "                model=\"prism-clip+7b\",\n",
    "            ),\n",
    "            EvaluationResult(\n",
    "                prediction_file=\"../data/gqa/prism-dinosiglip+7b/gqa-formatted-predictions.json\",\n",
    "                model=\"prism-dinosiglip+7b\",\n",
    "            ),\n",
    "            EvaluationResult(\n",
    "                prediction_file=\"../data/gqa/prism-siglip+7b/gqa-formatted-predictions.json\",\n",
    "                model=\"prism-siglip+7b\",\n",
    "            ),\n",
    "        ],\n",
    "        question_file=\"../data/gqa/questions.json\",\n",
    "        subscenario_keyword=\"structural_type\"\n",
    "    ),\n",
    "    BenchmarkConfig(\n",
    "        name=\"text-vqa\",\n",
    "        results=[\n",
    "            EvaluationResult(\n",
    "                prediction_file=\"../data/text-vqa/instructblip-vicuna-7b/results+rank-0.json\",\n",
    "                model=\"instructblip-vicuna-7b\",\n",
    "            ),\n",
    "            EvaluationResult(\n",
    "                prediction_file=\"../data/text-vqa/llava-v1.5-7b/results+rank-0.json\",\n",
    "                model=\"llava-v1.5-7b\",\n",
    "            ),\n",
    "            EvaluationResult(\n",
    "                prediction_file=\"../data/text-vqa/prism-clip+7b/results+rank-0.json\",\n",
    "                model=\"prism-clip+7b\",\n",
    "            ),\n",
    "            EvaluationResult(\n",
    "                prediction_file=\"../data/text-vqa/prism-dinosiglip+7b/results+rank-0.json\",\n",
    "                model=\"prism-dinosiglip+7b\",\n",
    "            ),\n",
    "            EvaluationResult(\n",
    "                prediction_file=\"../data/text-vqa/prism-siglip+7b/results+rank-0.json\",\n",
    "                model=\"prism-siglip+7b\",\n",
    "            ),\n",
    "        ],\n",
    "        question_file=\"../data/text-vqa/annotations-textvqa-full.json\",\n",
    "    ),\n",
    "    BenchmarkConfig(\n",
    "        name=\"pope\",\n",
    "        results=[\n",
    "            EvaluationResult(\n",
    "                prediction_file=\"../data/pope/instructblip-vicuna-7b/results+rank-0.json\",\n",
    "                model=\"instructblip-vicuna-7b\",\n",
    "            ),\n",
    "            EvaluationResult(\n",
    "                prediction_file=\"../data/pope/llava-v1.5-7b/results+rank-0.json\",\n",
    "                model=\"llava-v1.5-7b\",\n",
    "            ),\n",
    "            EvaluationResult(\n",
    "                prediction_file=\"../data/pope/prism-clip+7b/results+rank-0.json\",\n",
    "                model=\"prism-clip+7b\",\n",
    "            ),\n",
    "            EvaluationResult(\n",
    "                prediction_file=\"../data/pope/prism-dinosiglip+7b/results+rank-0.json\",\n",
    "                model=\"prism-dinosiglip+7b\",\n",
    "            ),\n",
    "            EvaluationResult(\n",
    "                prediction_file=\"../data/pope/prism-siglip+7b/results+rank-0.json\",\n",
    "                model=\"prism-siglip+7b\",\n",
    "            ),\n",
    "        ],\n",
    "        question_file=\"../data/pope/questions.json\",\n",
    "        subscenario_keyword=\"split\"\n",
    "    ),\n",
    "]\n",
    "train_size: int | float = 4\n",
    "device = \"cpu\"\n",
    "number_item: int = 100\n",
    "random_state: int = 42\n",
    "clustering: Literal[\"irt\", \"correct.\"] = \"irt\"\n",
    "p_irt: bool = True\n",
    "gp_irt = True\n",
    "epochs = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tinybm = TinyBenchmark(save_dir, balance=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:custom_tiny_bench.processor.benchmark_processor:Opening ../data/gqa/questions.json\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Opening ../data/gqa/instructblip-vicuna-7b/gqa-formatted-predictions.json\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Number of predictions: 12578\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Opening ../data/gqa/llava-v1.5-7b/gqa-formatted-predictions.json\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Number of predictions: 12578\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Opening ../data/gqa/prism-clip+7b/gqa-formatted-predictions.json\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Number of predictions: 12578\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Opening ../data/gqa/prism-dinosiglip+7b/gqa-formatted-predictions.json\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Number of predictions: 12578\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Opening ../data/gqa/prism-siglip+7b/gqa-formatted-predictions.json\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Number of predictions: 12578\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:[create_correctness_array] Shape of correctness array (5, 12578)\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Opening ../data/text-vqa/annotations-textvqa-full.json\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Opening ../data/text-vqa/instructblip-vicuna-7b/results+rank-0.json\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Number of predictions: 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: ['instructblip-vicuna-7b', 'llava-v1.5-7b', 'prism-clip+7b', 'prism-dinosiglip+7b', 'prism-siglip+7b']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:custom_tiny_bench.processor.benchmark_processor:Opening ../data/text-vqa/llava-v1.5-7b/results+rank-0.json\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Number of predictions: 5000\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Opening ../data/text-vqa/prism-clip+7b/results+rank-0.json\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Number of predictions: 5000\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Opening ../data/text-vqa/prism-dinosiglip+7b/results+rank-0.json\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Number of predictions: 5000\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Opening ../data/text-vqa/prism-siglip+7b/results+rank-0.json\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Number of predictions: 5000\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:[create_correctness_array] Shape of correctness array (5, 5000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: ['instructblip-vicuna-7b', 'llava-v1.5-7b', 'prism-clip+7b', 'prism-dinosiglip+7b', 'prism-siglip+7b']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 8880.97it/s]\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:After binarize, error is [0.00248 0.0033  0.00224 0.00312 0.00322]\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Opening ../data/pope/questions.json\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Opening ../data/pope/instructblip-vicuna-7b/results+rank-0.json\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Number of predictions: 8910\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Opening ../data/pope/llava-v1.5-7b/results+rank-0.json\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Number of predictions: 8910\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Opening ../data/pope/prism-clip+7b/results+rank-0.json\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Number of predictions: 8910\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Opening ../data/pope/prism-dinosiglip+7b/results+rank-0.json\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Number of predictions: 8910\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Opening ../data/pope/prism-siglip+7b/results+rank-0.json\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:Number of predictions: 8910\n",
      "INFO:custom_tiny_bench.processor.benchmark_processor:[create_correctness_array] Shape of correctness array (5, 8910)\n",
      "INFO:custom_tiny_bench.tiny_benchmark:[prepare_data] correctness_array.shape == (5, 26488)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: ['instructblip-vicuna-7b', 'llava-v1.5-7b', 'prism-clip+7b', 'prism-dinosiglip+7b', 'prism-siglip+7b']\n"
     ]
    }
   ],
   "source": [
    "tinybm.prepare_data(bm_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:41:05] config: model_type='multidim_2pl' epochs=2000              cli.py:109\n",
      "           priors='hierarchical' initializers=[] dims=5 lr=0.1                  \n",
      "           lr_decay=0.9999 dropout=0.5 hidden=100 vocab_size=None               \n",
      "           log_every=200 seed=42 deterministic=True                             \n",
      "           data_path: ../data/datasets/irt_val_dataset.jsonlines      cli.py:111\n",
      "           output directory: ../data/models/irt_val_model             cli.py:112\n",
      "[20:41:05] amortized: False                                       dataset.py:112\n",
      "[20:41:05] Vocab size: None                                       training.py:90\n",
      "           Training Model...                                          cli.py:116\n",
      "           args: {'device': 'cpu', 'num_items': 26488,           training.py:134\n",
      "           'num_subjects': 3}                                                   \n",
      "           Parsed Model Args: {'device': 'cpu', 'num_items':     training.py:147\n",
      "           26488, 'num_subjects': 3, 'priors': 'hierarchical',                  \n",
      "           'dims': 5, 'dropout': 0.5, 'hidden': 100,                            \n",
      "           'vocab_size': None}                                                  \n",
      "torch.Size([79464]) torch.Size([79464])\n",
      "Training Pyro IRT Model for 2000 epochs\n",
      "┏━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "┃ Epoch ┃ Loss        ┃ Best Loss   ┃ New LR ┃\n",
      "┡━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "│ 1     │ 661974.9746 │ 661974.9746 │ 0.1000 │\n",
      "│ 201   │ 150852.8629 │ 54188.5823  │ 0.0980 │\n",
      "│ 401   │ 50184.2069  │ 47024.7369  │ 0.0961 │\n",
      "│ 601   │ 54120.5577  │ 44795.1886  │ 0.0942 │\n",
      "│ 801   │ 45595.0356  │ 44772.1066  │ 0.0923 │\n",
      "│ 1001  │ 47984.8816  │ 43849.0714  │ 0.0905 │\n",
      "│ 1201  │ 51984.7533  │ 43449.1927  │ 0.0887 │\n",
      "│ 1401  │ 45168.5934  │ 43096.3877  │ 0.0869 │\n",
      "│ 1601  │ 43935.1899  │ 42810.9418  │ 0.0852 │\n",
      "│ 1801  │ 43916.4775  │ 42578.3354  │ 0.0835 │\n",
      "│ 2000  │ 47823.5873  │ 42073.7422  │ 0.0819 │\n",
      "└───────┴───��─────────┴─────────────┴────────┘[20:41:37] Train time: 31.820732831954956                             cli.py:122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:35<00:35, 35.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:41:40] config: model_type='multidim_2pl' epochs=2000              cli.py:109\n",
      "           priors='hierarchical' initializers=[] dims=10 lr=0.1                 \n",
      "           lr_decay=0.9999 dropout=0.5 hidden=100 vocab_size=None               \n",
      "           log_every=200 seed=42 deterministic=True                             \n",
      "           data_path: ../data/datasets/irt_val_dataset.jsonlines      cli.py:111\n",
      "           output directory: ../data/models/irt_val_model             cli.py:112\n",
      "[20:41:40] amortized: False                                       dataset.py:112\n",
      "[20:41:40] Vocab size: None                                       training.py:90\n",
      "           Training Model...                                          cli.py:116\n",
      "           args: {'device': 'cpu', 'num_items': 26488,           training.py:134\n",
      "           'num_subjects': 3}                                                   \n",
      "           Parsed Model Args: {'device': 'cpu', 'num_items':     training.py:147\n",
      "           26488, 'num_subjects': 3, 'priors': 'hierarchical',                  \n",
      "           'dims': 10, 'dropout': 0.5, 'hidden': 100,                           \n",
      "           'vocab_size': None}                                                  \n",
      "torch.Size([79464]) torch.Size([79464])\n",
      "Training Pyro IRT Model for 2000 epochs\n",
      "┏━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "┃ Epoch ┃ Loss         ┃ Best Loss    ┃ New LR ┃\n",
      "┡━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "│ 1     │ 1345971.0505 │ 1345971.0505 │ 0.1000 │\n",
      "│ 201   │ 93897.4791   │ 91786.1902   │ 0.0980 │\n",
      "│ 401   │ 83159.4607   │ 70431.8225   │ 0.0961 │\n",
      "│ 601   │ 78241.7024   │ 59617.3436   │ 0.0942 │\n",
      "│ 801   │ 63401.9859   │ 56377.8690   │ 0.0923 │\n",
      "│ 1001  │ 89140.0338   │ 54233.1555   │ 0.0905 │\n",
      "│ 1201  │ 70462.5129   │ 53310.3297   │ 0.0887 │\n",
      "│ 1401  │ 57258.5270   │ 53310.3297   │ 0.0869 │\n",
      "│ 1601  │ 54694.0281   │ 51003.3093   │ 0.0852 │\n",
      "│ 1801  │ 53681.5982   │ 50511.4264   │ 0.0835 │\n",
      "│ 2000  │ 53976.3705   │ 50511.4264   │ 0.0819 │\n",
      "��───────┴──────────────┴──────────────┴────────┘[20:42:24] Train time: 43.36441087722778                              cli.py:122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:22<00:00, 41.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:42:27] config: model_type='multidim_2pl' epochs=2000              cli.py:109\n",
      "           priors='hierarchical' initializers=[] dims=10 lr=0.1                 \n",
      "           lr_decay=0.9999 dropout=0.5 hidden=100 vocab_size=None               \n",
      "           log_every=200 seed=42 deterministic=True                             \n",
      "           data_path: ../data/datasets/irt_dataset.jsonlines          cli.py:111\n",
      "           output directory: ../data/models/irt_model                 cli.py:112\n",
      "[20:42:27] amortized: False                                       dataset.py:112\n",
      "[20:42:27] Vocab size: None                                       training.py:90\n",
      "           Training Model...                                          cli.py:116\n",
      "           args: {'device': 'cpu', 'num_items': 26488,           training.py:134\n",
      "           'num_subjects': 4}                                                   \n",
      "           Parsed Model Args: {'device': 'cpu', 'num_items':     training.py:147\n",
      "           26488, 'num_subjects': 4, 'priors': 'hierarchical',                  \n",
      "           'dims': 10, 'dropout': 0.5, 'hidden': 100,                           \n",
      "           'vocab_size': None}                                                  \n",
      "torch.Size([105952]) torch.Size([105952])\n",
      "Training Pyro IRT Model for 2000 epochs\n",
      "┏━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┓\n",
      "┃ Epoch ┃ Loss        ┃ Best Loss   ┃ New LR ┃\n",
      "┡━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━┩\n",
      "│ 1     │ 868049.2536 │ 868049.2536 │ 0.1000 │\n",
      "│ 201   │ 134485.4231 │ 97898.0106  │ 0.0980 │\n",
      "│ 401   │ 129368.7816 │ 76489.8524  │ 0.0961 │\n",
      "│ 601   │ 92783.5589  │ 73323.2969  │ 0.0942 │\n",
      "│ 801   │ 87276.8002  │ 67047.8425  │ 0.0923 │\n",
      "│ 1001  │ 75905.1233  │ 64691.5963  │ 0.0905 │\n",
      "│ 1201  │ 73161.3646  │ 64691.5963  │ 0.0887 │\n",
      "│ 1401  │ 78127.0728  │ 64471.0090  │ 0.0869 │\n",
      "│ 1601  │ 67364.3789  │ 63782.2816  │ 0.0852 │\n",
      "│ 1801  │ 67388.8689  │ 62808.9414  │ 0.0835 │\n",
      "│ 2000  │ 65985.4411  │ 62808.9414  │ 0.0819 │\n",
      "└───────┴───��─────────┴─────────────┴────────┘[20:43:14] Train time: 47.3905189037323                               cli.py:122\n"
     ]
    }
   ],
   "source": [
    "tinybm.train_irt(train_size, device, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:custom_tiny_bench.tiny_benchmark:Points for gqa: [ 2976    64  6709  4348  7073  2146  3701  7092  3354  4539  1517  5714\n",
      " 12342  9029  1666  4085  3024  6027  2054 11973  3205  7967  2144  2761\n",
      "  9039  3371  9876 11193  5340  1048  3117   234  9131 11282 10794 10040\n",
      " 11064 10386 10759  4148  7296  9207  6704  9385  3466 11103  8793  3895\n",
      "  7863 12318  5095  7182   286  2348  5635  7909  8356  3637  8549  7915\n",
      "  9064  1109  6289  5734 11383  5318  7372 10058  9676  3326 10765  9644\n",
      "  9684 11109  3695 11045  2217 11029  7441  7573  1042  2146  5541  3975\n",
      "  8280  1700 12041  9512  1599  1262  9212  2776  9865  6311  4428  4435\n",
      "  7718  6938  1906 10005]\n",
      "INFO:custom_tiny_bench.tiny_benchmark:Points for text-vqa: [1492 3747 1605  606 2790 4041 2238  859  903 4561 4236 4631 4316 3351\n",
      " 2121 1911 4335 1749 3018  272 1063 4716 1509 2068  893 2932 2949  572\n",
      " 3692 2733 4152  863  138 3409 3345 4871  925 1717 3656  147  643 4211\n",
      "  162 1792 4308 4233 1557 4104 4777 3448  474  686 4755 4428 2591 1816\n",
      " 4933 3810 1309 2455  189 4282 1370 4336 4686 2848 3579 2484 4182 4025\n",
      "  698 3691  627 1115 4094 4545 3745 1653 1185 4889 1288 4184  843 1947\n",
      " 3751 2516  626 4310 4407 3463 1202 1029 2377  141 1609  957 2347 3845\n",
      " 1034 2613]\n",
      "INFO:custom_tiny_bench.tiny_benchmark:Points for pope: [8021 1938 5433 4266 7870 5336 4188 2063 5620 5038 2398 2582 3270 5134\n",
      " 4579 8589 6489 5813 6654  762 7832  267 2685 1492 6291 1954  672 1652\n",
      " 4272 3716 8262 4102 8653 5419 4890  540 8382 1562 7808 8050 1987 7572\n",
      " 3846  477 8198 4871 8166  613 4564 5933 3332  834 7593 1205 7992 4268\n",
      " 4451    0 4340  210 5230 5523 4161 7750 5526 8908 3258 7118 1533  622\n",
      "  670 8872 6881 7922 4282 3412 6216 4833 1025 4998 8177  382 1829 5560\n",
      " 2468 5094 2303 6037 1791 3692 5448 6462  868 4242 8029 6762 6300 5490\n",
      " 1409 6162]\n",
      "INFO:custom_tiny_bench.tiny_benchmark:[Anchor points] scenario: gqa, avg. error: 0.0193194467\n",
      "INFO:custom_tiny_bench.tiny_benchmark:[Anchor points] scenario: text-vqa, avg. error: 0.0056000000\n",
      "INFO:custom_tiny_bench.tiny_benchmark:[Anchor points] scenario: pope, avg. error: 0.0096520763\n"
     ]
    }
   ],
   "source": [
    "anchor = tinybm.get_anchors(number_item, random_state, clustering= clustering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 44.83it/s]\n",
      "INFO:custom_tiny_bench.estimator:[Naive accuracy]: 0.620000\n",
      "INFO:custom_tiny_bench.estimator:[IRT] predicted score for 0_th model in gqa: 0.624742\n",
      "INFO:custom_tiny_bench.estimator:[Naive accuracy]: 0.660000\n",
      "INFO:custom_tiny_bench.estimator:[IRT] predicted score for 0_th model in text-vqa: 0.630800\n",
      "INFO:custom_tiny_bench.estimator:[Naive accuracy]: 0.800000\n",
      "INFO:custom_tiny_bench.estimator:[IRT] predicted score for 0_th model in pope: 0.883951\n",
      "INFO:custom_tiny_bench.estimator:[p-IRT] predicted score for 0_th model in gqa: 0.652167\n",
      "INFO:custom_tiny_bench.estimator:[p-IRT] predicted score for 0_th model in text-vqa: 0.601193\n",
      "INFO:custom_tiny_bench.estimator:[p-IRT] predicted score for 0_th model in pope: 0.886838\n",
      "INFO:custom_tiny_bench.estimator:[gp-IRT] predicted score for 0_th model in gqa: 0.627060\n",
      "INFO:custom_tiny_bench.estimator:[gp-IRT] predicted score for 0_th model in text-vqa: 0.629147\n",
      "INFO:custom_tiny_bench.estimator:[gp-IRT] predicted score for 0_th model in pope: 0.884649\n"
     ]
    }
   ],
   "source": [
    "res = tinybm.estimate_performance(p_irt=p_irt, gp_irt=gp_irt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on gqa\n",
      "Model 0\n",
      "    [Balanced true accruacy] 0.64406\n",
      "    [Random] scenario: gqa, avg. error: 0.03594\n",
      "    [IRT] scenario: gqa, avg. error: 0.01932\n",
      "    [p-IRT] scenario: gqa, avg. error: 0.00811\n",
      "    [gp-IRT] scenario: gqa, avg. error: 0.01700\n",
      "Result on text-vqa\n",
      "Model 0\n",
      "    [Balanced true accruacy] 0.62520\n",
      "    [Random] scenario: text-vqa, avg. error: 0.01480\n",
      "    [IRT] scenario: text-vqa, avg. error: 0.00560\n",
      "    [p-IRT] scenario: text-vqa, avg. error: 0.02401\n",
      "    [gp-IRT] scenario: text-vqa, avg. error: 0.00395\n",
      "Result on pope\n",
      "Model 0\n",
      "    [Balanced true accruacy] 0.87430\n",
      "    [Random] scenario: pope, avg. error: 0.04430\n",
      "    [IRT] scenario: pope, avg. error: 0.00965\n",
      "    [p-IRT] scenario: pope, avg. error: 0.01254\n",
      "    [gp-IRT] scenario: pope, avg. error: 0.01035\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# Random sampling \n",
    "qids = np.array(range(5000))\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "for scenario in tinybm.scenarios_position.keys():\n",
    "    random_points = rng.choice(qids, size=number_item, replace=False)\n",
    "    equal_weights = np.array([1/number_item for _ in range(number_item)])\n",
    "\n",
    "    Y_random = tinybm.test_data[:,tinybm.scenarios_position[scenario]][:,random_points]\n",
    "    Y_hat = (Y_random*equal_weights).sum(axis=1)\n",
    "    Balanced_true = (tinybm.balance_weights*tinybm.test_data)[:,tinybm.scenarios_position[scenario]].mean(axis=1) \n",
    "\n",
    "    print(f\"Result on {scenario}\")\n",
    "    for i in range(Balanced_true.shape[0]):\n",
    "        print(f\"Model {i}\")\n",
    "        print(f\"    [Balanced true accruacy] {Balanced_true[i]:.5f}\")\n",
    "        print(f\"    [Random] scenario: {scenario}, avg. error: {np.abs(Y_hat[i]-Balanced_true[i]):.5f}\")\n",
    "        print(f\"    [IRT] scenario: {scenario}, avg. error: {np.abs(res[0][scenario][i]-Balanced_true[i]):.5f}\")\n",
    "        print(f\"    [p-IRT] scenario: {scenario}, avg. error: {np.abs(res[1][scenario][i]-Balanced_true[i]):.5f}\")\n",
    "        print(f\"    [gp-IRT] scenario: {scenario}, avg. error: {np.abs(res[2][scenario][i]-Balanced_true[i]):.5f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
